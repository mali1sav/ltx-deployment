# Rebuild again with more timeout allocation
# RunPod Serverless Deployment: Lightricks/LTX-2

Generated by RunPod Deployment Agent

## Resource Requirements

| Resource | Minimum | Recommended |
|----------|---------|-------------|
| VRAM | 24GB | 48GB |
| System RAM | 32GB | 32GB |

**Suggested GPUs:** A40, L40S, A100

**Confidence:** 95%

### Reasoning
- Known model: LTX-2 Video
- LTX-2 is a video diffusion transformer. Base model needs ~20GB, but video generation requires temporal attention buffers. 24GB minimum for short clips, 48GB recommended for longer/higher-res generation.

### Quantization

Quantization is available for this model, which can reduce VRAM requirements to ~16GB.
To enable, modify the handler.py to load with quantization (e.g., `load_in_8bit=True` or `load_in_4bit=True`).


## Deployment Steps

### 1. Build Docker Image

```bash
docker build -t ltx-2-serverless .
```

### 2. Push to Container Registry

```bash
# Tag for your registry
docker tag ltx-2-serverless your-registry/ltx-2-serverless:latest

# Push
docker push your-registry/ltx-2-serverless:latest
```

### 3. Create RunPod Serverless Endpoint

1. Go to [RunPod Serverless](https://www.runpod.io/console/serverless)
2. Click "New Endpoint"
3. Select your container image
4. Configure:
   - **GPU Type:** A40
   - **Min Workers:** 0 (scale to zero)
   - **Max Workers:** 1+ (based on expected load)

### 4. Test Your Endpoint

#### cURL

```bash
curl -X POST https://api.runpod.ai/v2/YOUR_ENDPOINT_ID/runsync \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
  "input": {
    "prompt": "A cat walking on grass",
    "num_frames": 24,
    "height": 480,
    "width": 704,
    "num_inference_steps": 30
  }
}'
```

#### Python

```python
import runpod

runpod.api_key = "YOUR_API_KEY"
endpoint = runpod.Endpoint("YOUR_ENDPOINT_ID")

result = endpoint.run_sync({'prompt': 'A cat walking on grass', 'num_frames': 24, 'height': 480, 'width': 704, 'num_inference_steps': 30})
print(result)
```

## Files Included

- `Dockerfile` - Container build configuration
- `handler.py` - RunPod serverless handler
- `README.md` - This file

## Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `HF_MODEL_ID` | HuggingFace model ID | Lightricks/LTX-Video |
| `HF_HOME` | Model cache directory | `/app/models` |

### Weight Download Strategy

Current: **build_time**

- `build_time`: Weights downloaded during Docker build (faster cold starts)
- `runtime`: Weights downloaded on first request (smaller image)

To change, regenerate with `--weight-strategy runtime`.

## Troubleshooting

### Out of Memory (OOM)

1. Try a GPU with more VRAM
2. Enable CPU offloading in handler.py
3. Use quantization if available

### Slow Cold Starts

1. Use `build_time` weight strategy
2. Increase min workers to keep warm instances

### Model Loading Errors

1. Verify HuggingFace model ID is correct
2. Check if model requires authentication (set `HF_TOKEN`)
3. Review handler.py imports match your model
