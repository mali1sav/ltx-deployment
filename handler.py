"""
RunPod Serverless Handler for Lightricks/LTX-2
Generated by RunPod Deployment Agent
"""

import os
import sys
import runpod
import torch

sys.path.insert(0, "/app/repo")

MODEL = None
TOKENIZER = None
MODEL_ID = os.environ.get("HF_MODEL_ID", "Lightricks/LTX-Video")


def load_model():
    """Load the model and tokenizer (called once at startup)."""
    global MODEL, TOKENIZER
    
    if MODEL is not None:
        return MODEL, TOKENIZER
    
    print(f"Loading model from {MODEL_ID}...")
    
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    TOKENIZER = AutoTokenizer.from_pretrained(MODEL_ID)
    MODEL = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        torch_dtype=torch.float16,
        device_map="auto",
    )
    
    print("Model loaded successfully!")
    return MODEL, TOKENIZER


def handler(job):
    """
    RunPod handler function.
    
    Expected input format:
    {
        "input": {
            "prompt": "Your prompt here",
            "max_new_tokens": 256,
            "temperature": 0.7,
            "top_p": 0.9
        }
    }
    """
    try:
        job_input = job["input"]
        model, tokenizer = load_model()
        
        prompt = job_input.get("prompt", "")
        max_new_tokens = job_input.get("max_new_tokens", 256)
        temperature = job_input.get("temperature", 0.7)
        top_p = job_input.get("top_p", 0.9)
        
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
            )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        return {"generated_text": generated_text}
        
    except Exception as e:
        return {"error": str(e)}


runpod.serverless.start({"handler": handler})
